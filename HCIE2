// CH 5 \\

--->DB Optimization
-> Checking the Server Hardware
uname -a
->Checking the CPU
top  //and press 1
us:
ni:
sy:
wa:
st:
id:
// dway amana press Shift+M to sort the results by memory.
->check memory
[root@server1 ~]# free -h
->Checking the OS Version
[root@server1 ~]# cat /etc/os-release
->Checking the Network Status
[root@server1 ~]#sudo yum install nginx
[root@server1 ~]# sudo yum install sysstat
[root@ecs-444 ~]# sar -n DEV 3 2
-> Checking the I/O Status
[root@server1 ~]# iostat -xm 3 2
->query key parameter
openGauss=# show max_process_memory;
openGauss=# show shared_buffers;
openGauss=# show work_mem;

---> Optimizing SQL Queries Using a Vectorized Executor
create tb ,insert data
->Enable the time
openGauss=# \timing on
-> Disable the vectorized executor
openGauss=# show try_vector_engine_strategy;
openGauss=# set try_vector_engine_strategy to off;
-> Run the query statement
openGauss=# select count(1) from (select t1.a from vector_t1 t1 left join vector_t2 t2 on t1.a = t2.a);
-> Enable the vectorized
openGauss=# set try_vector_engine_strategy to force;
-> Restore parameter 
openGauss=# \timing off
openGauss=# reset try_vector_engine_strategy;

--->SMP-related
crerate tb,insert data,timeing on'ka
openGauss=# reset query_dop;
openGauss=# show query_dop;
-> run query in normal mode
openGauss=# select count(1) from smp_t1 where a > 100000;
->Set query_dop to 4 to enable parallel
openGauss=# set query_dop = 4;
/*/agar rest parameter wist time off ka enja restkarawa

--->GUC Parameter Setting and Plan Hints
create tb,insert data
->Run explain to print
openGauss=# explain select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
->Disable the costs display.
openGauss=# explain(costs off) select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
->Print an execution plan in verbose mode
openGauss=# explain(verbose) select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
-> execution plan in analyze
openGauss=# explain analyze select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
->Print the CPU usage.
openGauss=# explain(analyze on, CPU on) select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
->Print the usage of the buffer
openGauss=# explain(analyze on, buffers on) select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
->Specify the output format
openGauss=# set explain_perf_mode = 'normal';
openGauss=# explain(format json) select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
openGauss=# explain(analyze, format json) select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
->Print an execution plan in performance
openGauss=# explain performance select t1.a from exp_t1 t1 full join exp_t2 t2 on t1.a = t2.a where t1.a < 10000;
/*/if performance is used,u can drop the tables

--->Printing an Execution Plan in Rollback Mode
create tb,
-> Print an execution plan using INSERT in normal mode
openGauss=# explain insert into exp_fallback_t1 values(1, 1);
openGauss=# select * from exp_fallback_t1;
->using UPDATE
openGauss=# insert into exp_fallback_t1 values (1, 1);
openGauss=# select * from exp_fallback_t1;
-> using DELETE 
openGauss=# insert into exp_fallback_t1 values (1, 1);
openGauss=# explain delete from exp_fallback_t1;
openGauss=# select * from exp_fallback_t1;

->using INSERT in analyze mode.
openGauss=# explain analyze insert into exp_fallback_t1 values(100, 100);
-> UPDATE in analyze mode
openGauss=# explain analyze update exp_fallback_t1 set a = 200, b = 200;

--->Setting GUC Parameters to Optimize an Execution Plan
create 3 tb,
declare
n1 numeric := 100;
n2 numeric := 0;
n3 numeric := 100;
begin
while n1 > 0 loop
 n2 := 0;
 while n2 < 100 loop
 n3 := 100;
 while n3 > 0 loop
 insert into btm_t1 values(n1, n2, n3);
 insert into btm_t2 values(n1, n3, n2);
 insert into btm_t3 values(n2, n1, n3); 
n3 := n3 - 1;
 end loop;
 n2 := n2 + 1;
 end loop;
 n1 := n1 - 1;
end loop;
end;
/
->create index
create index idx_btm_11 on btm_t1 using btree (a);
create index idx_btm_12 on btm_t1 using btree (b);
create index idx_btm_13 on btm_t1 using btree (c);
create index idx_btm_21 on btm_t2 using btree (a);
create index idx_btm_22 on btm_t2 using btree (b);
create index idx_btm_23 on btm_t2 using btree (c);
create index idx_btm_31 on btm_t3 using btree (a);
create index idx_btm_32 on btm_t3 using btree (b);
create index idx_btm_33 on btm_t3 using btree (c);
->Disable enable_bitmapscan
openGauss=# set enable_bitmapscan to off;
openGauss=# explain (costs off) select * from btm_t1 t1, btm_t2 t2, btm_t3 t3
where t1.a = t2.b and t2.a = t3.c and t3.a = t1.c and t1.a = 1 and t2. a = 1 and t3.a = 1;
->enable bitmascan enja haman query bnusarawa

---> enable_tidscan
create tb,insert data like above , desable tidscan,
openGauss=# explain analyze select * from tid_t1 where ctid = '(0,10)'::tid;
openGauss=# set enable_tidscan to on;
openGauss=# explain analyze select * from tid_t1 where ctid = '(0,10)'::tid;

->enable_indexonlyscan
create tb,create index to tb,insert data,disable indexo
openGauss=# explain analyze select a, b from testidx_t1 where a = 10 and b = 20;
enable indexo
openGauss=# explain analyze select a, b from testidx_t1 where a = 10 and b = 20;
openGauss=# drop table testidx_t1;
openGauss=# reset enable_indexonlyscan;

--->Scenarios Where GUC Parameters Cannot Affect an Execution Plan
openGauss=# create table employee(id int, deptid int);
openGauss=# create table manager(id int, deptid int);
openGauss=# insert into employee values(1, 1), (2,1),(3,2),(4, 1), (5,2);
openGauss=# insert into manager values(1,1), (2,2),(3,1),(4,2);
openGauss=# set enable_nestloop to off;
openGauss=# explain SELECT * FROM employee e JOIN manager m ON e.deptid < m.deptid;
openGauss=# reset enable_nestloop;
openGauss=# drop table employee;

--->Using Plan Hints on an Execution Plan
openGauss=# create table t1(c1 number,c2 number,c3 number);
openGauss=# create table t2(c1 number,c2 number,c3 number);
openGauss=# insert into t1 values(generate_series(1,10000), 2 ,3);
openGauss=# insert into t2 values(generate_series(1,10000), 2, 3);
openGauss=# explain analyze select * from t1 join t2 on t1.c1 = t2.c2;
openGauss=# explain analyze select/*+ mergejoin(t1 t2) */ * from t1 join t2 on t1.c1 = t2.c2;
///drop tables

--->Poor Plan Hints Increase the Time Consumption
create,insert
openGauss=# explain analyze select * from t1 join t2 on t1.c1 = t2.c2;
openGauss=# explain analyze select/*+ nestloop(t1 t2) */ * from t1 join t2 on t1.c1 = t2.c2;

--->Optimizing openGauss Indexes
crete tb,insert data,timg on,
openGauss=# select * from t1 where a > 1000 and a < 1003;
create index on t1(a)
openGauss=# \timing on
openGauss=# select * from t1 where a > 1000 and a < 1003;
openGauss=# \timing off

--->Optimizing Table Structures
-> Selecting Proper Storage Types
openGauss=# \timing on
create two tb,insert data
openGauss=# explain analyze select * from row_t where a > 100 and a < 1000;
openGauss=# explain analyze select * from col_t where a > 100 and a < 1000;
->Selecting Proper Partition Types
create table student_t1 (
id int,
name varchar(20),
score int
) partition by range(id) (
partition s1p1 values less than (100000),
partition s1p2 values less than (200000)
);
create table student_t2 (
id int,
name varchar(20),
score int
) partition by hash(id) (
partition s1p1,
partition s1p2
);
openGauss=# insert into student_t1 values (generate_series(1,499999), 1, generate_series(50, 100));
openGauss=# insert into student_t2 values (generate_series(1, 499999), 1, generate_series(50, 100));
openGauss=# explain analyze select * from student_t1 where id = 9999;
openGauss=# explain analyze select * from student_t1 where id > 100 and id < 1000;  //range query

---> CBO
create tb,insert data genreate
openGauss=# explain analyze select a, b, c from test1 where a > 20000 and a < 200000;
openGauss=# vacuum analyze test1;     /*/vacuum perform
openGauss=# explain analyze select a, b, c from test1 where a > 20000 and a < 200000;
openGauss=# Delete from test1;
DELETE 5000000
openGauss=# insert into test1 values (generate_series(1, 5000), generate_series(1, 1000), 'hello' ||
random()); 
openGauss=# vacuum analyze test1;         /*/perform vacuum
--->Modifying Cost Parameters
create tb,insert generate data,
openGauss=# reset seq_page_cost;
RESET
openGauss=# show seq_page_cost;
openGauss=# explain select a, b, c from test1 where a > 20000 and a < 200000;
openGauss=# set seq_page_cost to 10;       /*/modify seqpage

--->Diagnosing SQL Performance
->Useing WDR
openGauss=# alter system set enable_wdr_snapshot to on;
openGauss=# select * from snapshot.snapshot;
create tb,insert generate data
openGauss=# select count(1) from test1;
openGauss=# select count(1) , b, max(a) from test1 group by b order by b limit 5;
openGauss=# select create_wdr_snapshot();
openGauss=# select * from snapshot.snapshot;
openGauss=# \a \t \o ~/my_wdr_report.html                          /*/Adjust the output format and specify the WDR output file.
select generate_wdr_report(3, 4, 'all', 'cluster', null);          /*/ Generate a WDR.
openGauss=# \o \a \t         /*/Restore to the initial output

---> Optimizing Subqueries
->Correlated Subqueries and Non-correlated Subqueries
openGauss=# create table t1 (a int, b int);
openGauss=# create table t2 (a int, b int);
openGauss=# insert into t1 values(generate_series(1, 100000), generate_series(1, 1000));
openGauss=# insert into t2 values(generate_series(1, 100000), generate_series(1, 1000));
openGauss=# \timing on
openGauss=# select t1.a, t1.b from t1 where t1.a in (select b from t2 where t2.b in (1, 2, 3, 4, 5));
openGauss=# explain select t1.a, t1.b from t1 where t1.a in (select b from t2 where t2.b in (1, 2, 3, 4,5));
openGauss=# select t1.a, t1.b from t1 where t1.a in (select b from t2 where t2.a = t1.a and t2.b in (1,2, 3, 4, 5));

--->SQL Rewriting
->Replace UNION with UNION ALL.
create table t1 (c1 int, c2 int);
create table t2 (c1 int, c2 int);
openGauss=# insert into t1 values(generate_series(1, 10000), generate_series(1, 1000));
openGauss=# insert into t2 values(generate_series(20000, 30000), generate_series(20000, 30000));
openGauss=# Explain analyze select c1 from t1 union select c1 from t2;           ///Print the SQL execution
openGauss=# Explain analyze select c1 from t1 union all select c1 from t2;

// CH 6 \\

---> Backup and Restoration 
-> export data from posgres
[omm@opengauss01 script]$ gs_dump -U omm -f /home/omm/MPPDB_backup.sql -p 26000 postgres -F p
->to export the full information of the postgres database. 
[omm@opengauss01 script]$ gs_dump -U omm -f /home/omm/MPPDB_backup.tar -p 26000 postgres -F t
[omm@opengauss01 script]$ gs_dump -U omm -f /home/omm/MPPDB_backup -p 26000 postgres -F d
->gs_dumpall to Export Data postgres
[omm@opengauss01 script]$ gs_dumpall -f /home/omm/MPPDB_backup.sql -p 26000
->Export the employees table in the public schema from the postgres database and save it to an SQL text file.
openGauss-# \q
[omm@]$ gs_dump -U omm -f /home/omm/MPPDB_backup_employees.sql -p 26000 postgres -n public -t employees -F p 
[omm@]$ gs_dump -U omm -f /home/omm/MPPDB_backup_departments.tar -p 26000 postgres -n public -t departments -F t
after drop this tabels
-> Use gsql to restore the employees table
[omm@]$ gsql -p 26000 postgres -r -f /home/omm/MPPDB_backup_employees.sql

-> Physical Backup and Restor
[omm@opengauss01 script]$ gs_backup -t backup --backup-dir=/home/omm/ --parameter
->View the backup
[omm@opengauss01 ~]$ cd /home/omm
[omm@opengauss01 ~]$ tar -vxf parameter.tar
[omm@opengauss01 ~]$ tar -xvf parameter_opengauss01.tar
[omm@opengauss01 ~]$ cd parameter_opengauss01
[omm@opengauss01 parameter_opengauss01]$ ls -l
-> Full Backup Using gs_basebackup
\l
create db,create tb, insert into 
----- >> \c DB name   /*/to change db 
[omm@opengauss01 ~]$ mkdir /home/omm/backup
[omm@opengauss01 ~]$ gs_basebackup -D /home/omm/backup/ -p 26000
[omm@opengauss01 backup]$ cd /home/omm/backup
[omm@opengauss01 backup]$ ls -l

--->Restoring Data from Backup Files
drop db,
[omm@opengauss01 backup]$ gs_om -t status --detail
[omm@opengauss01 backup]$ gs_ctl stop -D /opt/huawei/install/data/dn
/*/Back up the original data directory
[omm@opengauss01 backup]$ cd /opt/huawei/install/data/
[omm@opengauss01 data]$ mv dn dn_bak
/*/Copy the backup file to the original data directory.
[omm@opengauss01 data]$ mv /home/omm/backup .
[omm@opengauss01 data]$ ls -l
/*/Rename the backup directory with the name of the original data directory.
[omm@opengauss01 data]$ mv backup dn
[omm@opengauss01 data]$ ls -l
[omm@opengauss01 data]$ gs_ctl start -D /opt/huawei/install/data/dn
\l

/*/Using gs_probackup to Perform Incremental Backup and Restoration
[omm@opengauss01 data]$ gs_guc reload -N all -I all -c "enable_cbm_tracking=on"
[omm@opengauss01 data]$ gsql -d postgres -p 26000 -r
openGauss=# show enable_cbm_tracking;
[omm@opengauss01 data]$ mkdir -p /home/omm/gs_probackup
[omm@opengauss01 data]$ gs_probackup init -B /home/omm/gs_probackup/
/*/Add a backup instance
[omm@opengauss01 data]$ gs_probackup add-instance --instance og1 -B
/home/omm/gs_probackup/ -D /opt/huawei/install/data/dn
[omm@opengauss01 data]$ gs_probackup set-config --instance=og1 -B /home/omm/gs_probackup/ - d postgres -p 26000
openGauss=# \c testdb
testdb=# insert into table_in_mytbs_ts values('openGauss');
testdb=# insert into table_in_mytbs_ts values('GaussDB');
testdb=# select * from table_in_mytbs_ts;
/*/Perform a full backup.
[omm@opengauss01 data]$ gs_probackup backup -B /home/omm/gs_probackup/ --instance og1 -b 
[omm@opengauss01 data]$ gs_probackup show -B /home/omm/gs_probackup/

testdb=# insert into table_in_mytbs_ts values('openEuler');
testdb=# insert into table_in_mytbs_ts values('Euler');
testdb=# select * from table_in_mytbs_ts;
[omm@opengauss01 data]$ gs_probackup backup -B /home/omm/gs_probackup/ --instance og1 -b 
/*/Simulate the deletion
openGauss=# \c testdb
testdb=# drop table table_in_mytbs_ts;
testdb=# \q
/*/Shut down the database
[omm@opengauss01 data]$ gs_ctl stop -D /opt/huawei/install/data/dn

--->Point-In-Time Recovery (PITR)
[omm@opengauss01 /]$ exit
[root@opengauss01 ~]# mkdir /ogarchive
[root@opengauss01 ~]# chmod 777 /ogarchive/

[root@opengauss01 ~]# su omm
[omm@opengauss01 ~]$ cd /ogarchive/
[omm@opengauss01 ogarchive]$ ll
[omm@opengauss01 ogarchive]$ vi /opt/huawei/install/data/dn/postgresql.conf

[omm@opengauss01 root]$ gs_om -t restart
[omm@opengauss01 root]$ gsql -d postgres -p 26000 -r
openGauss=# show archive_mode;
openGauss=# show archive_command;

[omm@opengauss01 ogarchive]$ mkdir /home/omm/backup1
[omm@opengauss01 ogarchive]$ gs_basebackup -D /home/omm/backup1/ -p 26000

[omm@opengauss01 ogarchive]$ cd /opt/huawei/install/data/dn/pg_xlog/
[omm@opengauss01 pg_xlog]$ rm -rf *
[omm@opengauss01 ~]# gs_om -t stop
[omm@opengauss01 ogarchive]$ cd /opt/huawei/install/data/
[omm@opengauss01 data]$ mv dn dn_bak
[omm@opengauss01 data]$ mv /home/omm/backup1 .
[omm@opengauss01 data]$ mv backup1 dn
[omm@opengauss01 data]$ ll
/*/Copy the archive log files to the pg_xlog directory and restart the databas
[omm@opengauss01 ogarchive]$ cd /ogarchive/
[omm@opengauss01 ogarchive]$ cp * /opt/huawei/install/data/dn/pg_xlog/
[omm@opengauss01 ogarchive]$ gs_om -t start

// CH 7 \\

--->Advanced Features 
->Installing DBMind
[root@opengauss01 software]# chown omm:dbgrp -R /opt/software/
[root@opengauss01 ~]# su omm
[omm@opengauss ~]$ gs_guc reload -D /opt/huawei/install/data/dn -c 'password_encryption_type=1'

[omm@opengauss ~]$ gsql -p 26000 -d postgres -c 'create user dbmind_monitor password "openEuler@1234";'
[omm@opengauss ~]$ gsql -p 26000 -d postgres -c 'alter user dbmind_monitor monadmin;'
[omm@opengauss01 openGauss]$ gsql -p 26000 -d postgres -c 'grant all privileges to dbmind_monitor;'
[omm@opengauss01 openGauss]$ gs_guc reload -D /opt/huawei/install/data/dn -c 'password_encryption_type=2'
[omm@opengauss01 openGauss]$ gsql -p 26000 -d postgres -c 'create database metadatabase owner dbmind_monitor;

// CH 8 \\

--->An Error Is Reported or Threads Cannot Be Created in High Concurrency Scenarios

->Download the sysbench pressure test tool.
[root@standalone ~]# wget https://github.com/akopytov/sysbench/archive/1.0.20.zip
[root@standalone ~]# sed -i 's/gpgcheck=1/gpgcheck=0/g' /etc/yum.repos.d/openEuler.repo
[root@standalone ~]# yum -y install make automake libtool pkgconfig unzip libaio-devel
[root@standalone ~]# yum -y install postgresql-devel postgresql
[root@standalone ~]# unzip 1.0.20.zip
[root@standalone ~]# cd sysbench-1.0.20/
[root@standalone sysbench-1.0.20]# ./autogen.sh
[root@standalone sysbench-1.0.20]# ls -l configure
[root@standalone sysbench-1.0.20]# ./configure --prefix=/sysbench --with-pgsql --without-mysql
[root@standalone sysbench-1.0.20]# ls -l Makefile
[root@standalone sysbench-1.0.20]# make && make install
->Configure environment variables
export PATH=/sysbench/bin:$PATH
[root@standalone ~]# source /etc/profile
[root@standalone ~]# sysbench --version
[root@standalone ~]# su - omm # Switch to user omm.
[omm@standalone ~]$ cd /opt/huawei/install/data/dn/ # Go to the directory where the
configuration file is stored.
[omm@standalone dn]$ vim postgresql.conf # Edit the configuration file.
[omm@standalone dn]$ vim /opt/huawei/install/data/dn/pg_hba.conf
/*/Restart the database and create a database and user for the pressure test.
[omm@standalone dn]$ gs_om -t restart
[omm@standalone ~]$ gsql -d postgres -p 26000 -r
openGauss=# create user sbtest Sysadmin password 'openGauss@1234';
openGauss=# GRANT ALL PRIVILEGES TO sbtest;
openGauss=# create database sbtest;

->Reproduce the issue.
[omm@standalone ~]$ sysbench /sysbench/share/sysbench/oltp_read_write.lua --db-driver=pgsql --
pgsql-host=192.168.56.28 --pgsql-port=26000 --pgsql-user=sbtest --pgsqlpassword='openGauss@1234' --pgsql-db=sbtest --tables=10 --table-size=1000000 --threads=4 --
time=300 --report-interval=10 prepare 

[omm@standalone ~]$ gsql -d sbtest -p 26000 -r
sbtest=# \dt+
->Run phase: Start the pressure test.
[omm@standalone sysbench]$ sysbench /sysbench/share/sysbench/oltp_point_select.lua --dbdriver=pgsql --pgsql-host=192.168.56.28 --pgsql-port=26000 --pgsql-user=sbtest --pgsqlpassword='openGauss@1234' --pgsql-db=sbtest --tables=10 --table-size=1000000 --threads=20 --
time=300 --report-interval=10 run

->Rectify the fault.
[root@standalone ~]# ulimit -u
value=max(32768, Number of instances x 8192)
->Restart the server.
[root@standalone ~]# reboot
->Restart the database.
[omm@standalone ~]# gs_om -t start
[omm@standalone sysbench]$ sysbench /sysbench/share/sysbench/oltp_point_select.lua --dbdriver=pgsql --pgsql-host=192.168.56.28 --pgsql-port=26000 --pgsql-user=sbtest --pgsqlpassword='openGauss@1234' --pgsql-db=sbtest --tables=10 --table-size=1000000 --threads=20 --
time=300 --report-interval=10 run

--->: Failing to Connect to and Start Up a Database Due to a Network Fault
->Install Nginx.
[root@standalone ~]# yum install -y nginx
[omm@standalone ~]# gs_om -t stop
/*/Start the Nginx service.
[root@standalone ~]# /usr/sbin/nginx -c /etc/nginx/nginx.conf
[root@standalone sbin]# ps -ef| grep nginx
[root@standalone sbin]# netstat -anop|grep 26000
[omm@standalone ~]# gs_om -t start
[root@standalone ~]$ /usr/sbin/nginx -s reload
[root@standalone sbin]# netstat -anop|grep 80
[omm@standalone ~]$ gs_om -t start

---> An Error Reporting Insufficient Space Is Reported
When TPC-H Data Is Imported Using a Script

[root@standalone sysbench-1.0.20]# yum install -y git
//Switch to user omm and download the test dataset from the /home/omm/
git clone https://gitee.com/xzp-blog/tpch-kit.git
//Go to the dbgen directory and generate the makefile
cd /home/omm/tpch-kit/dbgen/
make -f Makefile
[omm@standalone ~] $ gsql -d postgres -p 26000 -r
openGauss=# CREATE DATABASE tpch;
openGauss=# \q
/*/Create eight test tables
[omm@standalone dbgen]$ gsql -d tpch -f dss.ddl -p 26000
/*/After logging in to the tpch database, you can see the following eight tables:
/*/Generate test data for the eight tables
cd /home/omm/tpch-kit/dbgen/

/*/Compile the data import script LoadData.sh.
for i in `ls *.tbl`; do
 table=${i/.tbl/}
 echo "Loading $table..."
 sed 's/|$//' $i > /tmp/$i
 gsql tpch -p 26000 -q -c "TRUNCATE $table"
 gsql tpch -p 26000 -c "\\copy $table FROM '/tmp/$i' CSV DELIMITER '|'"
done
[omm@standalone dbgen]$ chmod +x LoadData.sh #d
[omm@standalone dbgen]$ sh LoadData.sh
[omm@standalone dbgen]$ sh LoadData.sh

/*/Check the disk space
[root@standalone ~]# df -h

/*/Method 2: Reallocate the disk space of the /tmp directory and restart the server.
[root@standalone ~]# rm -rf /tmp/*
[root@standalone ~]# vim /etc/fstab

/*/Add the following content to the end of the file to reallocate space (15 GB) to the /tmp
tmpfs /tmp tmpfs nodev,nosuid,size=15G 0 0
/*/After the server is restarted, modify the LoadData.sh script in Step 7 and run the
[root@standalone ~]# df -h 
/*/As shown in the command output, the size of the /tmp directory has been changed to 15 
for i in `ls *.tbl`; do
 table=${i/.tbl/}
 echo "Loading $table..."
 mv ./$i /tmp/$i
 gsql tpch -p 26000 -q -c "TRUNCATE $table"
 gsql tpch -p 26000 -c "\\copy $table FROM '/tmp/$i' CSV DELIMITER '|'"
 rm -rf

/*/Run the script
[omm@standalone dbgen]$ sh LoadData.sh
